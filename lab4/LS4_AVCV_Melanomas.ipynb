{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AMYEebG58lLP"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"bLJG0raG8lLR"},"source":["## Audio Processing, Video Processing and Computer Vision\n","## Bachelor in Data Science and Engineering - Universidad Carlos III de Madrid\n","\n","# LAB SESSION 4: IMAGE CLASSIFICATION WITH CNNs\n","\n","# AUTOMATIC DIAGNOSTIC SYSTEM OF SKIN LESIONS FROM DERMOSCOPIC IMAGES\n","\n","\n","### Iván González Díaz, Miguel Ángel Fernández Torres\n","\n","\n","<center><img src='http://www.tsc.uc3m.es/~igonzalez/images/logo_uc3m_foot.jpg' width=400 /></center>"]},{"cell_type":"markdown","metadata":{"id":"rJKHl7-R8lLT"},"source":["In this practice we are going to build a skin lesion diagnosis system based on dermoscopic image analysis.\n","\n","## Part 0: The problem\n","\n","Before starting the practice, we will briefly describe the database that we will use and the problem we aim to address:\n","\n","Our goal is to develop a CNN providing an automatic diagnosis of cutaneous diseases from dermoscopic images. Dermoscopy is a non-invasive technique that allows the evaluation of the colors and microstructures of the epidermis, the dermoepidermal joint and the papillary dermis that are not visible to the naked eye. These structures are specifically correlated with histological properties of the lesions. Identifying specific visual patterns related to color distribution or dermoscopic structures can help dermatologists decide the malignancy of a pigmented lesion. The use of this technique provides a great help to the experts to support their diagnosis. However, the complexity of its analysis limits its application to experienced clinicians or dermatologists.\n","\n","In our scenario, we will consider 3 classes of skin lesions:\n","\n","- Malignant melanoma: Melanoma, also known as malignant melanoma, is the most common type of cancer, and arises from pigmented cells known as melanocytes. Melanomas typically occur on the skin and rarely elsewhere such as the mouth, intestines, or eye.\n","\n","- Seborrheic keratosis: it is a noncancerous (benign) tumor of the skin that originates from the cells of the outer layer of the skin (keranocytes), so it is a non-melanocytic lesion.\n","\n","- Benign nevus: a benign skin tumor caused by melanocytes (it is melanocytic)\n","\n","Figure 1 shows a visual example of the 3 considered lesions:\n","\n","![Image of ISIC](https://www.tsc.uc3m.es/~igonzalez/images/ISIC.jpg)\n","\n","The dataset has been obtained from the 'Internatial Skin Imaging Collaboration' (ISIC) file. It contains 2750 images divided into 3 sets:\n","- Training set: 2000 images\n","- Validation set: 150 images\n","- Test set: 600 images\n","\n","For each clinical case, two images are available:\n","- The dermoscopic image of the lesion (in the ‘images’ folder).\n","- A binary mask with the segmentation between injury (mole) and skin (in the 'masks' folder)\n","\n","Additionally, there is a csv file for each dataset (training, validation and test) in which each lines corresponds with a clinical case, defined with two fields separated by commas:\n","- the numerical id of the lesion: that allows to build the paths to the image and mask.\n","- the lesion label: available only for training and validation, being an integer between 0 and 2: 0: benign nevus, 1: malignant melanoma, 2: seborrheic keratosis. In the case of the test set, labels are not available (their value is -1).\n","\n","Students will be able to use the training and validation sets to build their solutions and finally provide the scores associated with the test set. This practice provides a guideliness to build a baseline reference system. To do so, we will learn two fundamental procedures:\n","\n","- 1) Process your own database with pytorch\n","- 2) Design a feedforward CNN that processess images and provides a diagnostic\n","- 2) Use a regular network that has been pretrained using a large-scale general purpose dataset and fine-tune it for our diagnostic problem\n","\n","## Part 1: Handling our custom dataset with PyTorch\n","Now we are going to study how we can load and process our custom dataset in pytorch. For that end, we are going to use the package ``scikit-image`` for reading images, and the package ``panda`` for reading csv files.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-CAQlwM8lLT"},"outputs":[],"source":["from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform, util\n","from sklearn import metrics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils, models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import time\n","import copy\n","from PIL import Image\n","import pdb\n","import random\n","import numpy.random as npr\n","\n","random.seed(42)\n","npr.seed(42)\n","torch.manual_seed(42)\n","torch.backends.cudnn.enabled = False\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"-FjAzIWA8lLU"},"source":["The first thing we need to do is to download and decompress the dataset on a local folder:\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qgatkuqL8lLU"},"outputs":[],"source":["#ONLY TO USE GOOGLE COLAB. Run this code only the first time you run this notebook and then comment these lines\n","#from shutil import copyfile\n","#from google.colab import drive\n","#import os, sys\n","#drive.mount('/content/drive')\n","#copyfile('/content/drive/My Drive/Docencia/2025_2026/AVCV/2025-26/Labs/LS4/Material/LS4_AVCV_DB.zip', './LS4_AVCV_DB.zip') #Copy db files to our working folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pV8SiuJH8lLV"},"outputs":[],"source":["#NOTE: Run this only once, in the machine where you want to run your code, then comment these lines\n","import zipfile\n","zipPath='./LS4_AVCV_DB.zip' #path of the 1st zip file\n","dataFolder='./data' #We extract files to the current folder\n","with zipfile.ZipFile(zipPath, 'r') as zip_ref:\n","    zip_ref.extractall(dataFolder)"]},{"cell_type":"markdown","metadata":{"id":"W9nE6aS28lLV"},"source":["Now let's read the indexed file and display data from image 65. The file structure is one row per image of the database, and two fields:\n","- Image ID (a 4-digit sequence, adding 0 to the left side if required)\n","- Label: 0 benign nevus, 1 melanoma, 2 seborrheic keratosis\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZQtMjwP8lLW"},"outputs":[],"source":["db = pd.read_csv('data/dermoscopyDBtrain.csv',header=0,dtype={'id': str, 'label': int})\n","\n","#We show inform\n","n = 65\n","img_id = db.id[65]\n","label = db.label[65]\n","\n","\n","print('Image ID: {}'.format(img_id))\n","print('Label: {}'.format(label))\n"]},{"cell_type":"markdown","metadata":{"id":"lyr8wERs8lLW"},"source":["Now, let's create a simple function to show an image.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TCNIgVJV8lLW"},"outputs":[],"source":["def imshow(image, title_str):\n","    if len(image.shape)>2:\n","        plt.imshow(image)\n","    else:\n","        plt.imshow(image,cmap=plt.cm.gray)\n","    plt.title(title_str)\n","\n","plt.figure()\n","imshow(io.imread(os.path.join('data/images/', img_id + '.jpg' )),'Image %d'%n)\n","plt.figure()\n","imshow(io.imread(os.path.join('data/masks/', img_id + '.png')),'Mask %d'%n)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sKGhJLGB8lLW"},"source":["### Class Dataset\n","\n","The class `` torch.utils.data.Dataset`` is an abstract class that represents a dataset.\n","\n","To create our custom dataset in pytorch we must inherit from this class and overwrite the following methods:\n","\n","- `` __len__`` so that `` len (dataset) `` returns the size of the dataset.\n","- `` __getitem__`` to support indexing `` dataset [i] `` when referring to sample $i$\n","\n","We are going to create the train and test datasets of our diagnostic problem. We will read the csv in the initialization method `` __init__`` but we will leave the explicit reading of the images for the method\n","`` __getitem__``. This approach is more efficient in memory because all the images are not loaded in memory at first, but are read individually when necessary.\n","\n","Our dataset is going to be a dictionary `` {'image': image, 'mask': mask, 'label': label} ``. You can also take an optional `` transform '' argument so that we can add pre-processing and data augmentation techniques.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlfzygBb8lLX"},"outputs":[],"source":["class DermoscopyDataset(Dataset):\n","    \"\"\"Dermoscopy dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir,transform=None,  maxSize=0):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path al fichero csv con las anotaciones.\n","            root_dir (string): Directorio raíz donde encontraremos las carpetas 'images' y 'masks' .\n","            transform (callable, optional): Transformaciones opcionales a realizar sobre las imágenes.\n","        \"\"\"\n","        self.dataset = pd.read_csv(csv_file,header=0,dtype={'id': str, 'label': int})\n","\n","        if maxSize>0:\n","            newDatasetSize=maxSize #maxSize muestras\n","            idx=np.random.RandomState(seed=42).permutation(range(len(self.dataset)))\n","            reduced_dataset=self.dataset.iloc[idx[0:newDatasetSize]]\n","            self.dataset=reduced_dataset.reset_index(drop=True)\n","\n","        self.root_dir = root_dir\n","        self.img_dir = os.path.join(root_dir,'images')\n","        self.mask_dir = os.path.join(root_dir,'masks')\n","        self.transform = transform\n","        self.classes = ['nevus', 'melanoma', 'keratosis']\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        #Leemos la imagen\n","        img_name = os.path.join(self.img_dir,self.dataset.id[idx] + '.jpg')\n","        image = io.imread(img_name)\n","        #Leemos la máscara\n","        mask_name = os.path.join(self.mask_dir,self.dataset.id[idx] + '.png')\n","        mask = io.imread(mask_name)\n","\n","        sample = {'image': image, 'mask': mask, 'label':  self.dataset.label[idx].astype(dtype=np.long)}\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"]},{"cell_type":"markdown","metadata":{"id":"1KtcMiUi8lLX"},"source":["We now instantiate the class to iterate over some samples to see what we generate.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"9M2Ji06e8lLX"},"outputs":[],"source":["train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n","                                    root_dir='data')\n","\n","fig = plt.figure()\n","\n","for i in range(len(train_dataset)):\n","    sample = train_dataset[i]\n","    print(i, sample['image'].shape, sample['label'])\n","\n","    ax = plt.subplot(1, 4, i + 1)\n","    plt.tight_layout()\n","    ax.set_title('Sample #{}'.format(i))\n","    ax.axis('off')\n","    plt.imshow(sample['image'])\n","\n","    if i == 3:\n","        plt.show()\n","        break"]},{"cell_type":"markdown","metadata":{"id":"0Jp_b7kb8lLX"},"source":["**IMPORTANT**\n","\n","The optional parameter ``maxSize`` in the constructor allows us to subsample the number of images and consequently reduce the size of the dataset. If not set included or maxSize=0, then the dataset will include all the images. This parameter is useful to train models over smaller datasets during hyperparameter validation and design phases. Working with less images reduces the training time at the expense of obtaining results that may deviate from those obtained with the full dataset size. Of course, the larger the training dataset, the more stable results but the larger the training time. Hence, it is up to the students the use of this parameter."]},{"cell_type":"markdown","metadata":{"id":"jAnDSzeE8lLX"},"source":["### Data Pre-Processing and Augmentation: Transforms\n","----------\n","\n","In the previously shown examples we can see that the size of the images is not the same. This would prevent to train a red convolutional neuronal, as the vast majority require fixed-size inputs. Furthermore, the image is not always adjusted to the lesion, and indeed, in some examples lesions are very small compared to the size of the image. It would then be desirable to adjust the input images so that the lesion covers almost the entire image.\n","\n","To do this, we are going to create some preprocessing code, focusing on 5 transformations:\n","\n","- ``CropByMask``: to crop the image using the lesion mask\n","- ``Rescale``: to scale the image\n","- ``RandomCrop``: to crop the image randomly, it allows us to augment the data samples with random crops\n","- ``CenterCrop``: to perform a central crop of the image with the indicated size (useful in test)\n","- ``ToTensor``: to convert numpy matrices into torch tensors (rearranging the axes).\n","\n","We will define them as callable classes instead of simple functions, as we will not need to pass the transform  parameters every time we call a method. To do this, we only have to implement the `` __call__`` method and, if necessary, the `` __init__`` method.\n","Then we can use a transformation with the following code:\n","\n","::\n","\n","    tsfm = Transform(params)\n","    transformed_sample = tsfm(sample)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5-saDVm8lLY"},"outputs":[],"source":["class CropByMask(object):\n","    \"\"\"Crop the image using the lesion mask.\n","\n","    Args:\n","        border (tuple or int): Border surrounding the mask. We dilate the mask as the skin surrounding\n","        the lesion is important for dermatologists.\n","        If it is a tuple, then it is (bordery,borderx)\n","    \"\"\"\n","\n","    def __init__(self, border):\n","        assert isinstance(border, (int, tuple))\n","        if isinstance(border, int):\n","            self.border = (border,border)\n","        else:\n","            self.border = border\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        h, w = image.shape[:2]\n","        #Compute the coordinates of the bounding box that contains the mask\n","        sidx=np.nonzero(mask)\n","        minx=np.maximum(sidx[1].min()-self.border[1],0)\n","        maxx=np.minimum(sidx[1].max()+1+self.border[1],w)\n","        miny=np.maximum(sidx[0].min()-self.border[0],0)\n","        maxy=np.minimum(sidx[0].max()+1+self.border[1],h)\n","        #Crop the image\n","        image=image[miny:maxy,minx:maxx,...]\n","        mask=mask[miny:maxy,minx:maxx]\n","\n","        return {'image': image, 'mask': mask, 'label' : label}\n","\n","class Rescale(object):\n","    \"\"\"Re-scale image to a predefined size.\n","\n","    Args:\n","        output_size (tuple or int): The desired size. If it is a tuple, output is the output_size.\n","        If it is an int, the smallest dimension will be the output_size\n","            a we will keep fixed the original aspect ratio.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","\n","        img = transform.resize(image, (new_h, new_w))\n","        msk = transform.resize(mask, (new_h, new_w))\n","\n","        return {'image': img, 'mask': msk, 'label' : label}\n","\n","class RandomCrop(object):\n","    \"\"\"Randomly crop the image.\n","\n","    Args:\n","        output_size (tuple or int): Crop size. If  int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","\n","        if h>new_h:\n","            top = np.random.randint(0, h - new_h)\n","        else:\n","            top=0\n","\n","        if w>new_w:\n","            left = np.random.randint(0, w - new_w)\n","        else:\n","            left = 0\n","\n","        image = image[top: top + new_h,\n","                     left: left + new_w]\n","\n","        mask = mask[top: top + new_h,\n","                      left: left + new_w]\n","\n","\n","        return {'image': image, 'mask': mask, 'label': label}\n","\n","class CenterCrop(object):\n","    \"\"\"Crop the central area of the image\n","\n","    Args:\n","        output_size (tupla or int): Crop size. If int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","        rem_h = h - new_h\n","        rem_w = w - new_w\n","\n","        if h>new_h:\n","            top = int(rem_h/2)\n","        else:\n","            top=0\n","\n","        if w>new_w:\n","            left = int(rem_w/2)\n","        else:\n","            left = 0\n","\n","        image = image[top: top + new_h,\n","                     left: left + new_w]\n","\n","        mask = mask[top: top + new_h,\n","                      left: left + new_w]\n","\n","\n","        return {'image': image, 'mask': mask, 'label': label}\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays into pytorch tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","\n","        # Cambiamos los ejes\n","        # numpy image: H x W x C\n","        # torch image: C X H X W\n","        image = image.transpose((2, 0, 1))\n","        image = torch.from_numpy(image)\n","        # A la máscara le añadimos una dim fake al principio\n","        mask = torch.from_numpy(mask)\n","        mask = mask.unsqueeze(0)\n","        label=torch.tensor(label,dtype=torch.long)\n","\n","        return {'image':image,\n","                'mask':mask,\n","                'label':label}\n","\n","class Normalize(object):\n","    \"\"\"Normalize data by subtracting means and dividing by standard deviations.\n","\n","    Args:\n","        mean_vec: Vector with means.\n","        std_vec: Vector with standard deviations.\n","    \"\"\"\n","\n","    def __init__(self, mean,std):\n","\n","        assert len(mean)==len(std),'Length of mean and std vectors is not the same'\n","        self.mean = np.array(mean)\n","        self.std = np.array(std)\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        c, h, w = image.shape\n","        assert c==len(self.mean), 'Length of mean and image is not the same'\n","        dtype = image.dtype\n","        mean = torch.as_tensor(self.mean, dtype=dtype, device=image.device)\n","        std = torch.as_tensor(self.std, dtype=dtype, device=image.device)\n","        image.sub_(mean[:, None, None]).div_(std[:, None, None])\n","\n","\n","        return {'image': image, 'mask': mask, 'label' : label}"]},{"cell_type":"markdown","metadata":{"id":"tflLTBpr8lLY"},"source":["### Exercise:\n","\n","Use the following code to apply the previous transforms and experiment with the values of the parameters to study their influence. Read the documentation at the begining of each class to understand the sintaxis of the input parameters.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCAuINhO8lLY"},"outputs":[],"source":["### Modify this code to make your experiments....\n","cmask = CropByMask(15)\n","scale = Rescale(256)\n","rcrop = RandomCrop(224)\n","ccrop = CenterCrop(224)\n","\n","# Apply each of the above transforms on sample.\n","fig = plt.figure(figsize=(7,7))\n","sample = train_dataset[65]\n","ax = plt.subplot(3,2, 1)\n","plt.tight_layout()\n","ax.set_title('original')\n","plt.imshow(sample['image'])\n","\n","for i, tsfrm in enumerate([cmask, scale, ccrop, rcrop]):\n","    transformed_sample = tsfrm(sample)\n","\n","    ax = plt.subplot(3, 2, i+2)\n","    plt.tight_layout()\n","    ax.set_title(type(tsfrm).__name__)\n","    plt.imshow(transformed_sample['image'])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BEDssGSu8lLZ"},"source":["#### Using package torchvision.transforms\n","\n","Package torchvision.trasforms comes with many useful methods that implement valuable transforms for data augmentation. Those transforms can be applied either to torch.tensors or to images. However, torchvision uses PIL library to read and process images, in contrast to the matrix representation of images used in scikit-image library (the library we have used in the previous methods). Furthermore, some of the transforms affect both to input images and skin-lesion masks, which avoid applying them to our samples. Hence, in order to use a torchvision transform we need to implement a pipeline that:\n","\n","- 1) First, convert the matrix-based images to PILImages\n","- 2) Apply the transform to PILImages\n","- 3) Convert PILImages to matrices\n","\n","And applies these to input images and, if necessary, to the binary skin-lesion masks. The following class ``TVCenterCrop`` implements the same functionality as the previous ``CenterCrop``, but using the corresponding method in torchvision. It is therefore a useful example if you plan to make use of torchvision transforms.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WmzDItG78lLZ"},"outputs":[],"source":["class TVCenterCrop(object):\n","    \"\"\"Crop the central area of the image. Example using the method in torchvision. Requires to\n","    internally convert from skimage (numpy array) to PIL Image\n","\n","    Args:\n","        output_size (tupla or int): Crop size. If int, square crop\n","\n","    \"\"\"\n","\n","    def __init__(self, size):\n","        self.CC=transforms.CenterCrop(size)\n","\n","    def __call__(self, sample):\n","        image, mask, label = sample['image'], sample['mask'],sample['label']\n","        pil_image=Image.fromarray(util.img_as_ubyte(image))\n","        pil_image=self.CC(pil_image)\n","        image=util.img_as_float(np.asarray(pil_image))\n","\n","        pil_mask=Image.fromarray(util.img_as_ubyte(mask))\n","        pil_mask=self.CC(pil_mask)\n","        mask=util.img_as_float(np.asarray(pil_mask))\n","\n","        return {'image': image, 'mask': mask, 'label': label}"]},{"cell_type":"markdown","metadata":{"id":"z3Edxtzj8lLZ"},"source":["Now, we apply to transform to check the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X9LJKLv8lLZ"},"outputs":[],"source":["# Apply each of the above transforms on sample.\n","fig = plt.figure()\n","sample = train_dataset[65]\n","ax = plt.subplot(1,2, 1)\n","plt.tight_layout()\n","ax.set_title('original')\n","plt.imshow(sample['image'])\n","\n","tvcc=TVCenterCrop(256)\n","\n","transformed_sample = tvcc(sample)\n","ax = plt.subplot(1, 2, 2)\n","plt.tight_layout()\n","ax.set_title('Center Crop transform using torchvision')\n","plt.imshow(transformed_sample['image'])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"KfIHrmUQ8lLZ"},"source":["### Exercise\n","\n","Implement other transforms using torchvision package. Take TVCenterCrop as guide.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PGGmu0JZ8lLZ"},"outputs":[],"source":["### Put your code here....\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4ug7JaVZ8lLZ"},"source":["### Composed Transforms\n","\n","Now let's apply the different transformations to our images.\n","\n","We will rescale the images so that their smallest dimension is 224 and then make random crops of size 224. To compose the transformations ``Rescale`` and ``RandomCrop`` we can use ``torchvision.transforms.Compose``, which is a simple callable class.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJSWIXm28lLZ"},"outputs":[],"source":["composed = transforms.Compose([CropByMask(15), Rescale(224),RandomCrop(224)])\n","\n","# Apply each of the above transforms on sample.\n","fig = plt.figure()\n","sample = train_dataset[65]\n","ax = plt.subplot(1,2, 1)\n","plt.tight_layout()\n","ax.set_title('original')\n","plt.imshow(sample['image'])\n","\n","transformed_sample = composed(sample)\n","ax = plt.subplot(1, 2, 2)\n","plt.tight_layout()\n","ax.set_title('Composed transform')\n","plt.imshow(transformed_sample['image'])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HeLzLgmn8lLa"},"source":["### Exercise:\n","\n","Use the previous code to test different combinations of transform and discuss the results."]},{"cell_type":"markdown","metadata":{"id":"WaWsoVgQ8lLa"},"source":["### Exercise:\n","\n","Data augmentation techniques are useful as long as they model random transforms that may happen in the real world, and so can be found in test. Hence, the choice of the data augmentation techniques to be applied is problem-dependent and may differ from one dataset to another. Have a look at the images in the dataset and try to figure out which transforms are appropriate for these problem. Imagine how a dermatologist takes a dermoscopic picture and which factors may differ from one capture to another. Your results in the challenge will strongly depend on your design!!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iDk6-do78lLa"},"outputs":[],"source":["### Put your code here....\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"H88rmwgw8lLa"},"source":["Iterating the dataset\n","-----------------------------\n","\n","We can now put everything together to create the train and test datasets with the corresponding transformations.\n","In summary, every time we sample an image from the dataset (during training):\n","- We will read the image and the mask\n","- We will apply the transformations and we will crop the image using a bounding box computed from the mask\n","- As the final cropping operation is random, we perform data augmentation during sampling\n","\n","We can easily iterate over the dataset with a ``for i in range`` loop.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MOW7S8zV8lLa"},"outputs":[],"source":["#Pixel means and stds expected by models in torchvision\n","pixel_mean=[0.485, 0.456, 0.406]\n","pixel_std=[0.229, 0.224, 0.225]\n","\n","#Train Dataset\n","train_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtrain.csv',\n","                                    root_dir='data',\n","                                    maxSize=500, ###IMPORTANT: maxSize=500 to speed-up the training process\n","                                                 ### Set to 0 or 200 for training your optimal model\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(224),\n","                                    RandomCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=pixel_mean,\n","                                    std=pixel_std)\n","                                    ]))\n","#Val dataset\n","val_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBval.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(224),\n","                                    CenterCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=pixel_mean,\n","                                    std=pixel_std)\n","                                    ]))\n","\n","#Test dataset\n","test_dataset = DermoscopyDataset(csv_file='data/dermoscopyDBtest.csv',\n","                                    root_dir='data',\n","                                    transform=transforms.Compose([\n","                                    CropByMask(15),\n","                                    Rescale(224),\n","                                    CenterCrop(224),\n","                                    ToTensor(),\n","                                    Normalize(mean=pixel_mean,\n","                                    std=pixel_std)\n","                                    ]))\n","\n","for i in range(len(train_dataset)):\n","    sample = train_dataset[i]\n","\n","    print(i, sample['image'].size(), sample['label'])\n","\n","    if i == 3:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"UNbGBCRE8lLa"},"source":["Finally, we have to create a dataloader allowing to:\n","\n","- Sample batches of samples to feed the network during training\n","- Shuffle data\n","- Load the data in parallel using multiple cores.\n","\n","``torch.utils.data.DataLoader`` is an iterator that provides all these features. An important parameter of the iterator is ``collate_fn``. We can specify how samples are organized in batches by choosing the most appropriate function. In any case, the default option should work fine in most cases.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"ch5gkk_f8lLa"},"outputs":[],"source":["#Specify training dataset, with a batch size of 8, shuffle the samples, and parallelize with 3 workers\n","#IMPORTANT!!!!! If you plan to run this in a Windows machine, set num_workers=0. Pytorch has a bug on windows\n","train_dataloader = DataLoader(train_dataset, batch_size=64,\n","                        shuffle=True, num_workers=3)\n","#Validation dataset => No shuffle\n","val_dataloader = DataLoader(val_dataset, batch_size=256,\n","                        shuffle=False, num_workers=3)\n","\n","#Test Dataset => => No shuffle\n","test_dataloader = DataLoader(test_dataset, batch_size=256,\n","                        shuffle=False, num_workers=3)\n","\n","\n","# Auxiliary function to visualize a batch\n","def show_batch(sample_batched):\n","    \"\"\"Mostramos las lesiones de un batch.\"\"\"\n","    images_batch, labels_batch = \\\n","            sample_batched['image'], sample_batched['label']\n","    batch_size = len(images_batch)\n","    im_size = images_batch.size(2)\n","    grid_border_size = 2\n","\n","    #Generamos el grid\n","    grid = utils.make_grid(images_batch)\n","    #Lo pasamos a numpy y lo desnormalizamos\n","    grid=grid.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    grid = std * grid + mean\n","    grid = np.clip(grid, 0, 1)\n","    plt.imshow(grid)\n","    plt.title('Batch from dataloader')\n","\n","for i_batch, sample_batched in enumerate(train_dataloader):\n","    print(i_batch, sample_batched['image'].size(),\n","          sample_batched['label'])\n","    plt.figure()\n","    show_batch(sample_batched)\n","    plt.axis('off')\n","    plt.ioff()\n","    plt.show()\n","\n","    #We show the data of the 3rd batch and stop.\n","    if i_batch == 3:\n","        break"]},{"cell_type":"markdown","metadata":{"id":"DqJX1-2Z8lLb"},"source":["## Part 2: Design and train a custom diagnosis network\n","In the second part of the practice we are designing a diagnosis network from scratch. For that end, we next provide a definition of a network which is very similar to LeNet-5, which aims to classifiy input images of size 32x32 px containing digits (MNIST dataset with 10 classes). However, in this case, we are not forced to work with such small images and, indeed, it is highly recommended to user larger sizes (e.g. 224x224 px as it represents the standard size for most well-known networks trained in Imagenet).\n","\n","In the following cell we provide the skeleton of a network called CustomNet, with the necessary methods: ` __init__` (constructor) and ``forward`` (forward method processes an image and provides a vector with the scores of the 10 classes considered in the problem of digit classification).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVW3aLyg8lLb"},"outputs":[],"source":["#Example network\n","class CustomNet(nn.Module):\n","    def __init__(self):\n","        # In the constructor we will specify those processing blocks which require learnable parameters\n","        # We just define them to ensure persistence during training, although they will remain disconnected\n","        # until we define a processing pipeline in forward method.\n","        super(CustomNet, self).__init__()\n","\n","        #2D convolution with 3 input channels, 16 output channels, and 5x5 filters\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        # Maxpooling with kernel 2x2\n","        self.pool = nn.MaxPool2d(2, 2)\n","        # 2D convolution with 16 input channels, 64 output channels, and 5x5 filters\n","        self.conv2 = nn.Conv2d(6, 16, 5)\n","        # Fully-connected layer which expects an input with 16 channels and a spatial dimension of 5x5 (before flatenning)\n","        # and produces 120 channels at the output\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n","        # Fully-connected layer which expects an input of 120 channels and produces an output of 84 channels\n","        self.fc2 = nn.Linear(120, 84)\n","        # Fully-connected layer which expects an input of 84 channels and produces an output with 10 classes\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    #In forward method we connect layers and define the processing pipeline of the network\n","    def forward(self, x):\n","\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        #Flatten method vectorizes the spatial output of size hxwxchannels into a vector of length h*w*channels\n","        #by setting the parameter to 1, we start to flatten in dim=1 and do not vectorize the dimension representing\n","        #the images in the batch\n","        x = x.flatten(1)\n","        #x = F.relu(self.fc1(x))\n","        #x = F.relu(self.fc2(x))\n","        #x = self.fc3(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"0inFnd6-8lLb"},"source":["#### Exercise:\n","\n","Modify the previous network and define a CNN that processes a batch of images (e.g. of size 224x224) and computes their outputs with the scores for the 3 classes that we consider in our problem (nevus, melanoma and keratosis).\n","\n","**IMPORTANT**: The structure of the network must be consistent in order to transform the input tensor  Nx3x224x224 (N the number of images in the batch, 3 the RGB channels, and 224 the spatial dimensions) into the tensor with the scores of each case, with size Nx3 (N the number of images in the batch and 3 the 3 classes of the problem). To do so, you have to adjust the architecture of the previous example, including the number of layers, the strides of each one, etc. so that it transforms the input tensor into the desired final vector. As an aid, the following code is provided that allows you to run the grid over a batch of images and see what the is shape of the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiSiLkYr8lLb"},"outputs":[],"source":["customNet = CustomNet() #we initialize the network\n","customNet.to(device) #copy the network to the device (gpu)\n","#Sample a batch of data and get images and labels\n","data=next(iter(train_dataloader))\n","inputs = data['image'].to(device).float()\n","labels = data['label'].to(device)\n","\n","batchSize = labels.shape\n","print('Size of tensor containing a batch of images is {}'.format(inputs.shape))\n","\n","#Lo pasamos por la red\n","with torch.set_grad_enabled(False):\n","    outputs = customNet(inputs)\n","    print('Size of the output tensor is {}'.format(outputs.shape))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-XXTICLJ8lLh"},"source":["**Hint**: Start commenting all lines in the forward method in except of the first convolutional layer, execute the previous cell and and you will see the shape of the output of that layer. Go uncommenting layer by layer to see the sequential output sizes to decide how many convolutional layers may be adequate and the dimensions of the first fully convolutional layer."]},{"cell_type":"markdown","metadata":{"id":"c0ZStZ6e8lLh"},"source":["## Part 3: Train and evaluate our CNN\n","\n","In the third part of the practice we are going to build a first automatic system for the diagnosis of skin lesions based on the network that we have created in the previous section. To train and evaluate the network, we need to define several elements, such as the performance metric, the loss function, the optimizer or the learning rate strategy. In the next sections we will configure these elements.\n","\n","### Performance Metric for evaluation\n","We will start by defining the metric we will use to evaluate our network. In particular, and following the instructions of the organizers of the original ISIC challenge, we will use the area under the ROC or AUC (https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve).\n","\n","AUC is a metric that avoids setting a specific threshold to make detections and is applied over the soft outputs of a **binary** classifier. By modifying the value of the threshold, we can build a ROC curve setting the False Positive Rate (FPR) in the y-axis and the True Positive Rate (TPR) in the x-axis. TPR is the proportion of positive cases than have been succesfully detected, whereas FPR is the number of false detections divided by the number of negatives.\n","\n","For low detection thresholds and an imperfect system, TPR will be high at the expense of a high FPR (as the system always says 1). For high threholds, the opposite situation happens. Once the ROC is built, the AUC measures the integral behind the curve, which is in the range [0,1].\n","\n","Although, at least theoretically, AUC can be lower than 0.5, in partice, the output of a baseline system that randomly decides 0 or 1 with equal probability obtains an AUC=0.5, so lower values are usually caused by bugs in the code (and could be avoided just by inverting the outputs of the system).\n","\n","The following figure shows some examples of AUC and ROC curves:\n","\n","\n","<center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1024px-Roc_curve.svg.png' width=384 /></center>\n","\n","As we have mentioned, AUC is a metric to evaluate binary problems (labels 0,1), and has the advantage of being independent of the detection threshold. It also behaves well against unbalanced problems, as it evaluates the ranking of the scores (their order with respect to the labels) and not their absolute values.\n","\n","Since our problem is multiclass, we will calculate 3 different AUCs:\n","- 1) AUC of binary problem melanoma vs all\n","- 2) AUC of the binary problem seborrheic keratosis vs all\n","- 3) AUC average of the previous two\n","\n","The following function computes AUCs from the complete database outputs:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtuJopRk8lLh"},"outputs":[],"source":["#Function that computes 2 AUCs: melanoma vs all and keratosis vs all\n","# scores is nx3: n is the number of samples in the dataset\n","# labels is nx1\n","# Function resturns an array with two elements: the auc values\n","def computeAUCs(scores,labels):\n","\n","    aucs = np.zeros((2,))\n","    #Calculamos el AUC melanoma vs all\n","    scores_mel = scores[:,1]\n","    labels_mel = (labels == 1).astype(int)\n","    aucs[0]=metrics.roc_auc_score(labels_mel, scores_mel)\n","\n","    #Calculamos el AUC queratosis vs all\n","    scores_sk = scores[:,2]\n","    labels_sk = (labels == 2).astype(int)\n","    aucs[1]=metrics.roc_auc_score(labels_sk, scores_sk)\n","\n","    return aucs"]},{"cell_type":"markdown","metadata":{"id":"yDNAhUlX8lLh"},"source":["### Loss function, optimizer, and learning rate strategy\n","\n","In the next cell we will define the 3 elements we need to train:\n","- The loss function, which will be a crossed entreopia.\n","- The optimizer, in which we will use a SGD with momentum, with certain values of lr and momentum\n","- A learning rate strategy, using a fixed step method, decreasing the lr by a factor of 10, every 7 epochs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QqoSUZyu8lLi"},"outputs":[],"source":["#Loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# SGD with momentum\n","optimizer_ft = optim.SGD(customNet.parameters(), lr=1e-3, momentum=0.9)\n","\n","# An lr strategy which decreases lr by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"]},{"cell_type":"markdown","metadata":{"id":"XaGQPY7u8lLi"},"source":["### Training function\n","\n","We continue by defining the function that we will use to train our classifier. Note the existence of 3 nested loops:\n","- Epoch loop: a main loop that goes through training epochs and then allows values to be taken from the evaluation metrics.\n","- Database loop: the train database is executed first, training the model, and then the validation database, only to obtain outputs and calculate evaluation metrics.\n","- Loop of iterations: in each iteration the forward, the evaluation of the loss, the backward and the updating of the weights in the optimizer are made."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JRUOSHRt8lLi"},"outputs":[],"source":["#train_model parameters are the network (model), the criterion (loss),\n","# the optimizer, a learning scheduler (una estrategia de lr strategy), and the training epochs\n","def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    numClasses = len(image_datasets['train'].classes)\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_aucs = np.zeros((2,)) #AUCs melanoma vs all, and keratosis\n","    best_auc = 0\n","    best_epoch = -1\n","\n","    #Loop of epochs (each iteration involves train and val datasets)\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","\n","\n","        # Cada época tiene entrenamiento y validación\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set the model in training mode\n","            else:\n","                model.eval()   # Set the model in val mode (no grads)\n","\n","\n","            #Dataset size\n","            numSamples = dataset_sizes[phase]\n","\n","            # Create variables to store outputs and labels\n","            outputs_m=np.zeros((numSamples,numClasses),dtype=float)\n","            labels_m=np.zeros((numSamples,),dtype=int)\n","            running_loss = 0.0\n","\n","            contSamples=0\n","\n","            # Iterate (loop of batches)\n","            for sample in dataloaders[phase]:\n","                inputs = sample['image'].to(device).float()\n","                labels = sample['label'].to(device)\n","\n","\n","                #Batch Size\n","                batchSize = labels.shape[0]\n","\n","                # Set grads to zero\n","                optimizer.zero_grad()\n","\n","                # Forward\n","                # Register ops only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward & parameters update only in train\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # Accumulate the running loss\n","                running_loss += loss.item() * inputs.size(0)\n","\n","                #Apply a softmax to the output\n","                outputs=F.softmax(outputs.data,dim=1)\n","                # Store outputs and labels\n","                outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n","                labels_m [contSamples:contSamples+batchSize]=labels.cpu().numpy()\n","                contSamples+=batchSize\n","\n","            #At the end of an epoch, update the lr scheduler\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            #Accumulated loss by epoch\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","\n","            #Compute the AUCs at the end of the epoch\n","            aucs=computeAUCs(outputs_m,labels_m)\n","\n","            #And the Average AUC\n","            epoch_auc = aucs.mean()\n","\n","            print('{} Loss: {:.4f} AUC mel: {:.4f} sk: {:.4f} avg: {:.4f}'.format(\n","                phase, epoch_loss, aucs[0], aucs[1], epoch_auc))\n","\n","            # Deep copy of the best model\n","            if phase == 'val' and epoch_auc > best_auc:\n","                best_auc = epoch_auc\n","                best_aucs = aucs.copy()\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_epoch = epoch\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best model in epoch {:d} val AUCs: mel {:4f} sk {:4f} avg {:4f}'.format(best_epoch,best_aucs[0],best_aucs[1],best_auc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"5ug7vANf8lLi"},"source":["### Fine-tuning of a pre-trained CNN\n","Once we have defined the training and evaluation functions, we will fine-tune AlexNet CNN using our database. In addition, we define the loss, the optimizer and the lr scheduler:"]},{"cell_type":"markdown","metadata":{"id":"H8biY0Ez8lLi"},"source":["### Specifying the data loaders\n","Now we are going to assign the dataloaders that we had previously to the arrays of train and val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2G0mPFW8lLi"},"outputs":[],"source":["image_datasets = {'train' : train_dataset, 'val': val_dataset}\n","\n","dataloaders = {'train' : train_dataloader, 'val': val_dataloader}\n","\n","dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","class_names = image_datasets['train'].classes\n"]},{"cell_type":"markdown","metadata":{"id":"kw1iJbF98lLi"},"source":["### Train your custom network\n","Finally, we can train the network for several epochs and see the train and validation results."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"A46x5BlX8lLi"},"outputs":[],"source":["#We fix seeds for reproducibility\n","random.seed(42)\n","npr.seed(42)\n","torch.manual_seed(42)\n","\n","customNet = train_model(customNet, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=5)"]},{"cell_type":"markdown","metadata":{"id":"eP8xlBSR8lLj"},"source":["#### Exercise:\n","\n","The current code trains the network for 5 epochs using SGD with momentum, with a learning rate strategy and an early stop strategy (taking the epoch that obtains the best results in validation). You can modify this setting by choosing other values ​​for parameters, optimizers, strategies, etc.\n"]},{"cell_type":"markdown","metadata":{"id":"ORgpZjt-8lLj"},"source":["#### Exercise\n","\n","The current code trains the network for 5 epochs and then returns the model that obtains the best results in validation (measured through the AUC). It could be necessary, in case the training times are long and you work with Google Colab (with limits on the duration of the session), to implement a save-and-resume strategy that allows you to train models through different sessions . You can find details on how to do it at:\n","\n","https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html"]},{"cell_type":"markdown","metadata":{"id":"X989jBs38lLj"},"source":["## Part 4: Fine-tuning a pre-trained model\n","\n","Now, instead of training a CNN designed by us from the beginning, we are going to do a fine-tuning of a network that has previously been trained for another task. As seen in theory, this is usually a good alternative when there is not a large amount of data to train (in proportion to the parameters to be learned).\n","\n","In particular, we are going to use the Alexnet CNN network, included in the ``torchvision`` package, with the weights that have been pre-trained in Imagenet.\n","\n","The difference between both cases is that ImageNet has 1000 categories of objects and our database has 3 (nevus, melanoma and seborrheic keratosis). Therefore, we will be able to reuse the entire network except the last layer, where the dimensions will be different from our problem. Therefore, once we load the model, we will overwrite the last layer with a new one, with randomly initialized weights, that meets the dimensions of our database.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJjEYy-r8lLj"},"outputs":[],"source":["# Load the model => With pretrained=True we load the optimal weights for ImageNet,\n","# a dataset that has 1000 object categories => We do fine-tuning for our problem with 3 clases\n","ftNet = models.alexnet(pretrained=True)\n","#Print model structure\n","print(ftNet)\n","#Get the number of features of the last hidden layer\n","num_ftrs = ftNet.classifier[6].in_features\n","# Adjust the last layer to produce scores for 3 classes\n","# Generate a new layer that substitutes the old one\n","num_classes = len(train_dataset.classes)\n","ftNet.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","# Show again the structure of the model with the modifications\n","print(ftNet)\n","ftNet = ftNet.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"-l8G84Hn8lLj"},"source":["### Re-define the optimizer for the new network\n","The previous one was used on the customNet parameters, so we have to define a new one on the new network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvd5KdqY8lLj"},"outputs":[],"source":["# SGD with momentum\n","optimizer_ft = optim.SGD(ftNet.parameters(), lr=1e-3, momentum=0.9)\n","\n","# An lr strategy which decreases lr by a factor of 0.1 every 7 epochs\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"]},{"cell_type":"markdown","metadata":{"id":"Onz4mcVM8lLj"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZDrT3DA8lLj"},"outputs":[],"source":["#We fix seeds for reproducibility\n","random.seed(42)\n","npr.seed(42)\n","torch.manual_seed(42)\n","\n","ftNet = train_model(ftNet, criterion, optimizer_ft, exp_lr_scheduler,\n","                       num_epochs=5)"]},{"cell_type":"markdown","metadata":{"id":"HktDviXE8lLj"},"source":["#### Exercise\n","\n","The current solution uses Alexnet pre-trained on ImageNet and then fine-tunes it for our particular problem. You can try other networks, many of which come pre-trained in the ``torchvision`` package. For this, it is always necessary to adapt the last layers to our database, generating 3-position outputs. You can find a guide at:\n","\n","https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n","\n","In addition to the torchvision models, github and other repositories also have very competitive networks that could be useful.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"R4nPmAK88lLk"},"source":["## Part 5: Evaluation (Important)\n","The evaluation of this practice will be done through a challenge, for which the students will have to send the results based on the test data in two categories:\n","\n","- 1) CUSTOM Category: The results on the test set of a custom network, created from scratch by the students. In this case, the complete code of the network must appear in cells of the notebook and it will not be possible to make use of external networks/packages or pre-trained models.\n","\n","- 2) FINE-TUNING Category: The results on the test set of a network that has been previously initialized in another database (with fine-tuning). In this case, existing models in torchvision or even external networks can be used.\n","\n","In addition, the final mark will depend both on the results in both categories and on the content of a brief report (1 side for the description, 1 side for extra material: tables, figures and references) where they will describe the most important aspects of the proposed solutions. The objective of this report is for the teacher to assess the developments/extensions/decisions made by the students when optimizing their system. You do not need to provide an absolute level of detail about the changes made, just list them and briefly discuss the purpose of the changes.\n","\n","### Details of the material to send for evaluation:\n","\n","One zip file will be uploaded per group with the following contents:\n","\n","- Two .csv files with the test outputs of the models that have trained in the two categories. Each file will contain a matrix of size 600x3, 600 lesions and the 3 classes considered in the problem. The array should be provided in csv format (with 3 numbers per row separated by ','). Code to generate the outputs is provided later.\n","\n","- The report as described above.\n","\n","- The notebook that integrates the creation of both models so that the teacher can check how things have been implemented.\n","\n","**The deadline for submission of your results + the report is Friday Oct 31 at 23:59.**\n"]},{"cell_type":"markdown","metadata":{"id":"sASUav6X8lLk"},"source":["Next we provide some functions that allow to test the network and create the csv file with the outputs.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OprM0qF88lLk"},"outputs":[],"source":["### Code that generates the test matrix\n","def test_model(model):\n","    since = time.time()\n","\n","    numClasses = len(test_dataset.classes)\n","\n","    model.eval()   # Ponemos el modelo en modo evaluación\n","\n","    #Tamaño del dataset\n","    numSamples = len(test_dataset)\n","\n","    # Creamos las variables que almacenarán las salidas y las etiquetas\n","    outputs_m=np.zeros((numSamples,numClasses),dtype=np.float)\n","    labels_m=np.zeros((numSamples,),dtype=int)\n","    contSamples=0\n","\n","    # Iteramos sobre los datos\n","    for sample in test_dataloader:\n","        inputs = sample['image'].to(device).float()\n","\n","\n","        #Tamaño del batch\n","        batchSize = inputs.shape[0]\n","\n","        # Paso forward\n","        with torch.torch.no_grad():\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","\n","            #Aplicamos un softmax a la salida\n","            outputs=F.softmax(outputs.data,dim=1)\n","            outputs_m [contSamples:contSamples+batchSize,...]=outputs.cpu().numpy()\n","            contSamples+=batchSize\n","\n","\n","    return outputs_m"]},{"cell_type":"markdown","metadata":{"id":"xKRl3skV8lLk"},"source":["Running the previous function and obtaining the matrix with the scores (Nx3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dtr7T_gI8lLk"},"outputs":[],"source":["outputs=test_model(ftNet)"]},{"cell_type":"markdown","metadata":{"id":"TB3tH1388lLk"},"source":["And finally save the matrix into a csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MfrtyipJ8lLk"},"outputs":[],"source":["import csv\n","\n","with open('output_test.csv', mode='w') as out_file:\n","    csv_writer = csv.writer(out_file, delimiter=',')\n","    csv_writer.writerows(outputs);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hW3KvDkk8lLk"},"outputs":[],"source":["print(outputs)"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}